---
title: "Solutions"
output:
  html_document:
    toc: yes
    df_print: paged
    number_sections: yes
    toc_float: yes
    code_folding: hide

knit: ( 
  function(input_file, encoding) {
    out_dir <- 'docs';
    
    rmarkdown::render(
    input       = input_file,
    encoding    = encoding,
    output_file = file.path(dirname(input_file), out_dir, 'solutions.html') ) })
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = T,
                      eval = T,
                      include = T,
                      message = FALSE,
                      warning = FALSE,
                      results = "hide")

```


# Imai, Kim and Wang 2022 {#ImaiKimWang}

["Matching methods for Causal Inference with Time-Series Cross-Sectional Data" by Imai, K., Kim, I.S. and Wang, E.H. (2022)](https://doi.org/10.1111/ajps.12685)

R-package: [PanelMatch()](https://github.com/insongkim/PanelMatch).

$~$

## The problem it solves

Most matching methods are not fit for Time-Series Cross-Sectional (TSCS) data. The few solutions that exist assume [**staggered adoption**](definitions.html). They create a matched DiD-application for TSCS-data without this assumption.

They address staggered adoption both in the sense that units are treated at unequal T, but also that there is a Average Causal effect of Reversal (ART), so that units jump back and forth in being treated. To our knowledge, this is the only solution that seamlessly integrates both of these effects.


$~$

## The way it solves it

Their proposed solution is best understood as a three-stage process. In stage 1 we construct, for each treated unit, a matched set of control units that share identical treatment status over an adjustable time period [**L**](definitions.html). In stage 2 we refine these matched sets of units by balancing over observed, time-varying confounders. In stage 3 we execute the DiD-estimator.

```{r preperation, eval = TRUE}

library(tidyverse);library(PanelMatch);library(did)
data(dem) # Example data from "Democracy Does Cause Growth" by Acemoglu, Naidu, Restrepo, and Robinson (2019)

```

$~$

### Stage 1: Constructing matched sets

In the first stage, units that experience the treatment are matched with units that do not experience the treatment (control). This is practically an exact-match procedure, but relying only on the units' treatment status over a user-specified length of time. This length of time is called [**L**](definitions.html) (short for Lags). If L = 0, units are matched solely on treatment status at time T, but this this would no longer be utilizing the time-dimension in the dataset.

In figure \@ref(fig:ExampleStage1) they illustrate an example with 5 units over 6 time periods, with L = 3. Values in the triangles and circles indicate whether this is a treated unit or a control unit, while the square box is the pre-treatment period, the length of which is defined by [**L**](definitions.html). In creating matched sets, units with similar pre-treatment periods *across the exact same time periods T* but with unequal treatment status in the triangles/circles are matched together to compose a set. In the left-hand panel (a), we are looking for units we can use to find the [**ATT**](definitions.html). The three red units and the three blue units compose separate matched sets with one treated unit and two control units. The grey units compose a third matched set with one treated unit and one control unit.

The right-hand panel (b) is similar, but now we are looking for sets with which we can analyse the [**ART**](definitions.html). The blue circle in this panel simply illustrates a case of reversal but which do not have any suitable match in the data.

Any data point that does not enter into a matched set is discarded.

$~$

![(#fig:ExampleStage1) An Example of Matched Sets with Five Units and Six Time Periods and with L = 3](docs/Graphics/ajps12685-fig-0002-m.jpg)

$~$

The following R-code gives an overview of the `PanelMatch()`-arguments that refer to stage 1:

```{r Stage1_Arguments, eval = FALSE}


mod <- PanelMatch(
  
  #General information for the PanelMatch-function:
  data = dem, #Data to be used
  time.id = "year", #Name of time-column 
  unit.id = "wbcode2", #Name of unit-column
  treatment = "dem", #Name of treatment variable
  outcome.var = "y", #Name of outcome variable (they have named the column Y...lol...)
  qoi = "att", #Quantity of interest.
  verbose = TRUE, #Prints more information about all calculations
  
  #    Arguments for Stage 1:
  matching = TRUE, #Should Stage 1 exact match on treatment history?
  lag = 4, # This argument controls the number of lags, notated as "L" int he article
  match.missing = FALSE, # Should patterns of missingness be included in matching the units into sets?
  listwise.delete = TRUE, #Delete missingness listwise
  exact.match.variables = NULL, #In principle, the exact matching in stage 1 can include all categorical variables, not just the treatment history. Character vector of relevant categorical variables can be added here. NULL means that we will only match on treatment history.
  
  ...
  
)

```

$~$

### Stage 2: Refine match sets

In stage 2 the goal is to establish balance on time-varying covariates within these matched sets over the pre-treatment period ([**L**](definitions.html)). This is to address the [**parallel trends assumption**](definitions.html). This can be done with many existing approaches, and the article suggests either using a matching procedure such as Mahalanobis distance or Propensity matching, or create unit weights.

In the evaluations in the article, weighting performs better, and at times a lot better, than distance matching.

#### Distance-balancing

*Using Mahalanobis-distance:* Computes standardized Mahalanobis-distance between the treated unit and each of its matched control units, averaged over the pre-treatment period.

*Propensity score matching:* Estimates the conditional probability that a unit will experience the treatment given pre-treatmeant covariates (the propensity score). This approach requires additional specification for how to estimate the probability of being treated, such as a logistic regression.

Once a distance measure has been selected and computed, the sample is reduced to the **J** most similar units within each matched set.

#### Weight-balancing

Each control unit within each matched set is assigned a weight, with greater weight assigned to more similar units. This approach requires the user to select some method to determine similarity. The authors suggest [propensity score weighting](https://onlinelibrary.wiley.com/doi/10.1111/1468-0262.00442) or calibration weights.

$~$

The following R-code gives an overview of the `PanelMatch()`-arguments that refer to stage 2:

```{r Stage2_Arguments, eval = FALSE}


mod <- PanelMatch(
  
  #General information for the PanelMatch-function:
  data = dem, #Data to be used
  time.id = "year", #Name of time-column 
  unit.id = "wbcode2", #Name of unit-column
  treatment = "dem", #Name of treatment variable
  outcome.var = "y", #Name of outcome variable (they have named the column Y...lol...)
  qoi = "att", #Quantity of interest.
  verbose = TRUE, #Prints more information about all calculations
  
  #    Arguments for Stage 1:
  ...
  
  #    Arguments referring to Stage 2:
  lead = 0:4, #How far (years) into the future should the outcome be measured? Notated as F in the article
  forbid.treatment.reversal = FALSE, #Should reversal be allowed in the period between the treatment occurs (T) and the outcome is measured (T + leads/F)
  refinement.method = "CBPS.weight", #Covariate balancing propensity score weights from Imai and Ratkovic (2014)
  size.match = 5, #Indicates the J closest matches to keep in the refinement. This is only relevant for the distance-methods, not weight-methods.
  covs.formula = as.formula(~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)) #Formula for refinement
                            
  )
  
```

$~$

### Stage 3: The DiD-estimator

Before executing the DiD-estimator, the user needs to specify how long into the future the effect should occur, called [**F**](definitions.html). If F = 2 and treatment occurs in T = 0, then the outcome Y from T+2 is used. This is similar to lagging the treatment variable in a normal regression setup, but with a lead-logic instead. Once F is specified, we can proceed with the DiD-estimator.

The DiD-estimator sequence: 1. Calculates the counterfactual outcome for each treated unit using the weighted average of the refined matched control units. 2. Computes DiD estimate of the ATT for each treated unit. 3. Averages the ATT estimates across all treated units.

$~$

The following R-code first runs stage 1 and 2 with the `PanelMatch()`-function, then executes stage 3 using the `PanelEstimate`-function. It also produces a simply visualization of the result:

$~$

```{r FullFunction, eval = TRUE}
      
      
      mod <- PanelMatch(
        
        #General information for the PanelMatch-function:
        data = dem, #Data to be used
        time.id = "year", #Name of time-column 
        unit.id = "wbcode2", #Name of unit-column
        treatment = "dem", #Name of treatment variable
        outcome.var = "y", #Name of outcome variable (they have named the column Y...lol...)
        qoi = "att", #Quantity of interest.
        verbose = TRUE, #Prints more information about all calculations
        
        #    Arguments for Stage 1:
        matching = TRUE, #Should Stage 1 exact match on treatment history?
        lag = 4, # This argument controls the number of lags, notated as "L" int he article
        match.missing = FALSE, # Should patterns of missingness be included in matching the units into sets?
        listwise.delete = TRUE, #Delete missingness listwise
        exact.match.variables = NULL, #In principle, the exact matching in stage 1 can include all categorical variables, not just the treatment history. Character vector of relevant categorical variables can be added here. NULL means that we will only match on treatment history.
        
        #    Arguments referring to Stage 2:
        lead = 0:4, #How far (years) into the future should the outcome be measured? Notated as F in the article
        forbid.treatment.reversal = FALSE, #Should reversal be allowed in the period between the treatment occurs (T) and the outcome is measured (T + leads/F)
        refinement.method = "CBPS.weight", #Covariate balancing propensity score weights from Imai and Ratkovic (2014)
        size.match = 5, #Indicates the J closest matches to keep in the refinement. This is only relevant for the distance-methods, not weight-methods.
        covs.formula = as.formula(~ I(lag(tradewb, 1:4)) + I(lag(y, 1:4)) ) #Formula for refinement
        
      )
      
      #Stage 3
      res <- PanelEstimate(sets = mod, data = dem)
      
      #Simple visualization of the result
      plot(res)
      
      
```
$~$

### Extensions
Step 1 of the `PanelMatch()`-approach is intuitive and rather straight-forward. While it is currently not implemented in the `R`-package, one the initial matching and balancing has been done, we could estimate the ATT with Poisson, logit, or other distributions.


## Unsolved problems

-   Requires binary treatment

$~$

## List of assumptions

-   Parallel trends

-   Assumed causal order: Confounders Z are realized before treatment X is realized before outcome Y

-   Assumes absence of between-unit spillover effects (they are looking into applications without this assumption)



# Callaway and Sant'Anna 2021

["Difference-in-Differences with Multiple Time Periods" by Callaway, Brantly and Sant'Anna, Pedro H. C. (2021)](https://www.sciencedirect.com/science/article/abs/pii/S0304407620303948)

## The problem it solves

Callaway and Sant'Anna provide a way to estimate the ATT under staggered treatment and heterogeneous treatment effects. The [did](https://cran.r-project.org/web/packages/did/vignettes/did-basics.html) R-package includes a simple to use approach to correctly estimate the group-time average treatment effects. Also see the [did webpage](https://bcallaway11.github.io/did/) for more information.

## The way it solves it

By avoiding bad control groups (e.g., comparing treated units with already treated units, or if one thinks that the never-treated group is different from the group that eventually becomes treated, one can also only use the not yet treated group as control). Provides ways to aggregate the treatment effects into group-time treatment effects, the average group-level treatment effect, or the average effect of participating in treatment for groups that have been exposed to treatment exactly $n$ time periods. The default approach is a parametric doubly robust approach (using both inverse probability weighting and regression estimators). See ["Potential Outcomes, Treatment Effects, and Estimation Methods"](https://www.dropbox.com/sh/zj91darudf2fica/AADWlJIH9SI3XvtADXgYma0ka/ESTIMATE_DiD?dl=0&preview=ESTIMATE_TRF_0.mp4&subfolder_nav_tracking=1) for an explanation of what "doubly robust" means (at the end of the lecture).

This code implements the method:

```{r, message=FALSE, warning=FALSE, fig.height=12}
    source("Data/generate_panel_data.R") #Generate random data

    out <- att_gt(yname = "y",
    gname = "first_treat",
    idname = "id",
    tname = "time",
    xformla = ~1,
    data = dat,
    est_method = "reg",
    control_group = "notyettreated" #Will include units that will receive treatment later as control group
    )
    
    #Plot
    ggdid(out, ylim = c(-.25,.1))
    
    #Event-type plot
    es <- aggte(out, type = "dynamic")
    ggdid(es)
    
```

Event-study plot:

```{r, message=FALSE, warning=FALSE, height=12}
    #Event-type plot
    es <- aggte(out, type = "dynamic")
    ggdid(es)
    
```


# Wooldridge 2021 {#wooldridge}

["Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators" by Wooldridge, Jeffrey M. (2021)](https://www.researchgate.net/publication/353938385_Two-Way_Fixed_Effects_the_Two-Way_Mundlak_Regression_and_Difference-in-Differences_Estimators)

Wooldridge's [Dropbox folder](https://t.co/q1AnkhEF97) (Contains Stata-code)

[The Tweet](https://twitter.com/jmwooldridge/status/1427472491367305219)

## The problem it solves

Wooldridge shows in this article that the issue with staggered intervention and heterogeneous treatment effects can be solved by altering the parameterization of the linear regression model. He also shows that you can get similar results both using TWFE or a pooled model where group level averages are included into the model (the Two-Way Mundlak model). The benefit with the latter approach is that it is easily adapted to non-linear models such as exponential, logit, and probit models. This idea is further elaborated in another article: ["Simple Approaches to Nonlinear Difference-in-Differences with Panel Data"](https://www.researchgate.net/publication/362532821_Simple_Approaches_to_Nonlinear_Difference-in-Differences_with_Panel_Data). Wooldridge has also recorded a ["lecture on this topic"](https://www.dropbox.com/sh/zj91darudf2fica/AADj_jaf5ZuS1muobgsnxS6Za?dl=0&preview=did_seminar_20210923.mp4) (link to his shared Dropbox folder). The introduction to ["Potential Outcomes, Treatment Effects, and Estimation Methods"](https://www.dropbox.com/sh/zj91darudf2fica/AADWlJIH9SI3XvtADXgYma0ka/ESTIMATE_DiD?dl=0&preview=ESTIMATE_TRF_0.mp4&subfolder_nav_tracking=1) also comes highly recommended.


## The way it solves it

TWFE equation for non-staggered homogeneous treatment:
\begin{equation}
Y_{it} = \alpha + \beta w_{it} + c_i + g_t + u_{it}, t = 1, ..., T; i = 1,2,...,N
\end{equation}
TWM equation non-staggered homogeneous treatment:
\begin{equation}
Y_{it} = \alpha + \beta w_{it} + d_i + p_t + u_{it}, t = 1, ..., T; i = 1,2,...,N
\end{equation}

where $d_i = 1$ if eventually treated (are you in the treated or control group), $p_t = 1$ if a post-treatment period (are you in pre- or post-treatment period), and $w_{it} = d_i \cdot p_t$

TWM equation for staggered heterogeneous treatment:
\begin{equation}
Y_{it} = \alpha + \beta_q (w_{it} \cdot fq_t) + ... + \beta_T (w_{it} \cdot fT_t) + \xi d_i + \theta_q fq_t + ... + \theta_T fT_t  + u_{it}, t = 1, ..., T; i = 1,2,...,N
\end{equation}
where $p_t = fq_t + ... + fT_t = 1$ ($fq_t$ being an indicator for a specific post-treatment period)

Knowing about this kind of approach, although more cumbersome for estimating the standard cohort effect using e.g., Callaway and Sant'Anna's approach, could be useful e.g., if you want to specify cohort specific covariate effects.


## An example implementation of TWM for real TSCS data in R

For applied researchers like ourselves, implementing Wooldridge's TWM was not straight forward. Here, we give a step-by-step guide for how to setup the data and estimate the ATT, using the dataset from [Acemoglu, Naidu, Restrepo and Robindson (2019)](https://www.journals.uchicago.edu/doi/10.1086/700936). This is the same dataset used as an example for `PanelMatch()` above.

In doing this, we want the code to be able to accommodate all kinds of complexities: staggered adoption, treatment reversal, heterogeneous treatment effects, and the presence of never-treated and always-treated units.


```{r twm_example_preperation, message=FALSE, warning=FALSE}
library(PanelMatch);library(fixest)
data(dem)


```


The dataset comes as a common country-year long-format table. The variables of interest for us is the unit ID (`wbcode2`), year (`year`), and whether or not country `i` is a democracy (`dem`) at time `t`. We need some more stuff:

1. A time-until/since-treatment-occurred variable, with separate categories from always- and never-treated
2. Cohort dummies that identifies:
    + units that are treated in the same year
    + units that were already treated when time-series started
    + units that are never treated
3. Deal with reversal, by one of two ways:
    + Delete all years for a unit from the year it reverses, or
    + Separate units into multiple spells, with a new spell created for a unit each time it reverses to non-treated status
        + **_NOTE:_**  Separating units into multiple spells at reversal is not something Wooldridge suggests, but we do not see any reason why it shouldn't work. We can cluster the standard errors by country (and not country-spell) to deal with non-independent error terms.



To get the list of items above, we first need to identify whether and in what year a unit is treated. We can do this by simply comparing democracy status in t with t-1. But in doing this, we need to fix another complication: We can't have missing data, as this will ruin later code. When lagging, we will have missing data in the first year. If you are willing to assume that treatment-status did not change in the first year, you can simply assign the same status code to democracy in t-1 is it has in t.

```{r twm_lag_dem, message=FALSE, warning=FALSE}

#####     Goal 1: Identify when a unit is treated
#Lag dem by 1 to identify when change occurs
dem <- dem[order(dem$wbcode2, dem$year),] #Order before lag
dem <- dem %>% group_by(wbcode2) %>% mutate(dem_lag1 = lag(dem))

#We can't have missing data, as this will ruin later code. But if you are willing to assume that treatment-status did not change in the first year, you can run this code
dem$dem_lag1 <- ifelse(is.na(dem$dem_lag1)==TRUE, dem$dem,
                       dem$dem_lag1) #Assume that treatment status was equal to first year


```


We still need to remove rows with missing information on our treatment (`dem`). A word of caution: Make sure that this does not delete time-series in the middle of a country's history, only at start or end. You must avoid this:

```{r twm_na_problematic_situation, message=FALSE, warning=FALSE}
error_example <- data.frame("ID" = 1,
                            "year" = 1970:1973,
                            "dem_lag1" = c(1, NA, NA, 1))

knitr::kable(error_example)

```


You can diagnose whether this is a problem. If this code returns any FALSE, you need to inspect:

```{r diagnose_time_jump, message=FALSE, warning=FALSE}
table(dem %>% subset(is.na(dem_lag1)==FALSE) %>%
        group_by(wbcode2) %>%
        mutate(duration_since_last_year = 1==(year - lag(year, 1)) ) %>% ungroup() %>%
        select(duration_since_last_year))

```

Once this is corrected, you can delete rows:

```{r twm_remove_na, message=FALSE, warning=FALSE}
###     If diagnose is ok, you can delete rows
dem <- dem[which(is.na(dem$dem_lag1)==FALSE),]

```


Now you can identify change in treatment-status. Here we create the variable `change_in_dem`, which can have the values -1 (reversal), 0 (status quo), and 1 (treated).

```{r twM_change_in_dem, message=FALSE, warning=FALSE}
###     Identify change in treatment status
dem$change_in_dem <- dem$dem - dem$dem_lag1 #change_in_dem can now take the values -1 (reversal), 0 (status quo), and 1 (treated)

```

Now we wncounter point 4 above: How should we deal with reversal? We start by identifying the occurrences of reversal, and assign unit-spell IDs, so that each unit has a new ID for each time it reverses:

```{r twm_identify_reversal, message=FALSE, warning=FALSE}

#####     Deal with reversal
###     If you want to include the same country multiple times, it must be considered a new unit each time it reverses back to none-treated status:
dem$reversal <- ifelse(dem$change_in_dem < 0, 1, 0)
dem <- dem %>% group_by(wbcode2) %>% mutate(spell = cumsum(reversal)) #Count number of reversals
dem$unit_spell_id <- paste0(dem$wbcode2, "_", dem$spell) #By combining unit-id with number of reversals, we now have a unit-spell-ID.

```

If you want to go with solution 1 to the reversal problem, to simply delete all country-years once a country reverses, run the following code. If you want to keep countries as separate units after each reversal, do not run the following code.

```{r twm_delete_reversals, message=FALSE, warning=FALSE, eval = FALSE}
#If you want to delete everything after reversal, do this:
dem <- dem[which(dem$spell<1),]

```

Right, 1/3 points dealt with. We now continue on to identify when treatment occurs for a unit, and then assign all units to a cohort dependeing on when treatment occurred:

```{r twm_identify_treatment, message=FALSE, warning=FALSE}
#####     Identify points in time where treatment occurs
dem$treated_this_year <- ifelse(dem$change_in_dem < 0, 0, dem$change_in_dem) #Converts -1 to 0


#####     Identify cohort
dem <- dem[order(dem$unit_spell_id, dem$year), ]
tmp_cohortdat  <- dem[which(dem$treated_this_year==1), c("unit_spell_id", "year")]
tmp_cohortdat  <- tmp_cohortdat %>% rename(cohort = year) #Units will now be given cohort ID equal to the year they were treated
dem <- full_join(dem, tmp_cohortdat)
rm(tmp_cohortdat)

```

The keen eye will notice that the above code does not assign any cohort to never-treated and always-treated units, because `treated_this_year` is always 0 for these units. This is because we want to create a time-until/since-treatment-occurred (point 2 above) before we do this. This is easier to do when we can treat `cohort` as a numeric variable, which we can't once we assign cohorts to never-treated and always-treated (they do not have any meaningful treatment-year).

Here, we create `time_to_traetment`, which is our time-until/since-treatment-occurred-variable.

```{r twm_create_time_relative_variable, message=FALSE, warning=FALSE}
###     Create relative-time-to-treatment: (we need to adjust this later but easier to create while we can keep "cohort" numeric)
dem$time_to_treatment <- dem$year-dem$cohort #This is easier to create while we can have cohort numeric (which we can't once we give a cohort to never- and always-treated)
```

We can now assign cohort to always- and never- treated units:

```{r twm_cohort_to_never_always, message=FALSE, warning=FALSE}
###     Give cohort to always- and never-treated
dem <- dem %>% mutate(cohort = ifelse(is.na(cohort)==TRUE & dem==1, "always_treated",
                                       ifelse(is.na(cohort)==TRUE & dem==0, "never_treated", as.character(cohort) ) ) )
```

And we also need to give values to always- and never-treated units on the `time_to_treatment`-variable.

```{r twm_relative_time_to_never_always, message=FALSE, warning=FALSE}
###     Give relative-time also to never- and always- treated
dem$time_to_treatment <- ifelse(dem$cohort == "never_treated", "Infinity",
                                ifelse(dem$cohort == "always_treated", "-Infinity", as.character(dem$time_to_treatment)
                                ))
```


This is some obsolete code that we don't think is needed, but it creates Wooldridge's _w_:
```{r twm_w, message=FALSE, warning=FALSE}
###     Indicate if unit will ever be (or always was) treated
dem$eventually_treated <- ifelse(dem$cohort=="never_treated", 0, 1) #This code is probably obsolete but not really sure. Useful to produce Wooldridge's w-variable. 

#####     Create the Wooldridge's w
dem$w <- dem$eventually_treated * dem$dem #w_i_t = d_i * p_t = (will unit be treated) * (are you in post-treatment-period)


```


And we have solved all points on our list. We can now get all sorts of estimates with simple regression. The power of the Mundlak-approach is that it flexible for different distributions, e.g. gaussian, poisson, logit etc.

Here, we estimate the cohor-specific ATT, and show that it is quite dramatically heterogeneous:

```{r twm_att_heteroplot, message=FALSE, warning=FALSE, fig.height=12, fig.width=24}

#####   Estimate ATT
mod <- feols(y ~ factor(cohort) : factor(time_to_treatment) | year , data = dem, cluster = ~wbcode2)
#####     Graph
mod <- tidy(mod, conf.int = TRUE)

###   Clean term to get the useful information
mod$cohort <- gsub(".*cohort.(.*):factor.*", "\\1", mod$term)
mod$time_to_treatment <- as.numeric(gsub(".*time_to_treatment.(.*)$", "\\1", mod$term))

### Remove always-treated from plot
mod <- mod %>% subset(time_to_treatment>-Inf)

### Plot
ggplot(mod, aes(y = estimate, ymin = conf.low, ymax = conf.high, x = time_to_treatment, fill=cohort)) +
  geom_line() +
  geom_ribbon(alpha = 0.5) +
  geom_vline(xintercept = 0) +
  theme_minimal()

```

Lots of heterogeneity, but is there a signal in there somewhere? We can get the mean for each point in `time_to_treatment`, which shows the clear effect of democracy on GDP:

```{r twm_time_average, message=FALSE, warning=FALSE, fig.height=12, fig.width=24}
mean_mod <- mod %>% group_by(time_to_treatment) %>% mutate(mean_est = mean(estimate),
                                                           mean_max = mean(conf.high),
                                                           mean_low = mean(conf.low))
ggplot(mean_mod, aes(x = time_to_treatment, y = mean_est, ymax = mean_max, ymin = mean_low)) +
  geom_ribbon(alpha = 0.7) +
  geom_line() +
  geom_vline(xintercept = 0) +
  theme_minimal()
```


### Extensions
There are various non-paramteric alteration we could make to the data. For example, we could restrict it to only include a certain number of years before and after treatment occurs. We could add a quarantine period for observations after treatment reverses, if we want units to be able to re-enter the dataset after reversal, but not right away.